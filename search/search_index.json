{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Rad-MLOps","text":""},{"location":"#who-we-are","title":"Who We Are","text":"<p>We are a dynamic duo: a condensed matter physics PhD turned deep-learning medical imaging scientist, and an MD turned AI solutions architect. With over 10 years of combined industry experience, we aim to document our journey of implementing MLOps in medical imaging the right way.</p>"},{"location":"#mission","title":"Mission","text":"<p>Our mission is to teach you how to deploy and monitor medical imaging ML models within regulated environments using MLOps best practices.</p>"},{"location":"#goals","title":"Goals","text":"<ol> <li>Develop strategies for productionizing ML models in regulated environments, focusing on MLOps best practices.</li> <li>Understand the do\u2019s and don\u2019ts of MLOps within various medical regulatory constraints (e.g., FDA, CE).</li> <li>Identify and provide solutions to common roadblocks related to the CI/CD of ML solutions for medical imaging tasks.</li> <li>Organize code snippets and scripts showcasing common libraries and tools used in medical imaging MLOps.</li> <li>Share these learnings and tips with our community here!</li> </ol>"},{"location":"#key-concepts","title":"Key Concepts","text":"<ol> <li>Data Management</li> <li>CI/CD (Continuous Integration/Continuous Deployment)</li> <li>Automated Testing</li> <li>Model Training &amp; Testing</li> <li>Model Deployment</li> <li>Model Monitoring</li> </ol>"},{"location":"index_1/","title":"DIFFERENT to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"index_1/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"index_1/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"index_1/#code-annotation-examples","title":"Code Annotation Examples","text":"<p>Testing writing <code>code</code> here.</p> <pre><code>Testing writing code block here\n</code></pre> <p>Testing <code>py</code> code here</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>adding file name as title</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>adding line numbers</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>Highlighting lines</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>Emojis</p> <p>\ud83d\ude04</p> <p></p> <p></p>"},{"location":"index_2/","title":"DIFFERENT to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"index_2/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"index_2/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"index_2/#code-annotation-examples","title":"Code Annotation Examples","text":"<p>Testing writing <code>code</code> here.</p> <pre><code>Testing writing code block here\n</code></pre> <p>Testing <code>py</code> code here</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>adding file name as title</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>adding line numbers</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>Highlighting lines</p> <pre><code>import tensorflow as tf\ndef just_a_function():\n    ASSET_DIR = './'\n    asset = 'test'\n</code></pre> <p>Emojis</p> <p>\ud83d\ude04</p> <p></p> <p></p>"},{"location":"blog/regulatory/","title":"Quality and Regulatory Documentation","text":""},{"location":"blog/regulatory/#user-needs-performance-requirements","title":"User Needs &amp; Performance Requirements","text":""},{"location":"blog/regulatory/#performance-testing-and-surveillance-plan-and-procedure","title":"Performance Testing and Surveillance: Plan and Procedure","text":""},{"location":"blog/regulatory/#data-ingestion-management-plan-and-procedure","title":"Data Ingestion &amp; Management: Plan and Procedure","text":""},{"location":"blog/regulatory/#performance-improvement-plan-and-procedure","title":"Performance Improvement: Plan and Procedure","text":""},{"location":"blog/deployment/latency_and_throughput/","title":"Latency and Throughput","text":"<p>When deploying a machine learning model for medical imaging, latency and throughput must be considered to ensure both performance and compliance. Medical imaging models often need to process large files quickly. When integrating a model into production code, it is good practice to benchmark the integrated solution to ensure it meets the required performance metrics for both single-image processing time.</p>"},{"location":"blog/deployment/latency_and_throughput/#example-dockerized-brain-mri-tumor-detection-model","title":"Example: Dockerized Brain MRI Tumor Detection Model","text":"<p>Let's consider a concrete example of latency and throughput benchmarking for a brain MRI tumor detection model packaged in a Docker container:</p>"},{"location":"blog/deployment/latency_and_throughput/#model-specifications","title":"Model Specifications:","text":"<ul> <li>Task: Binary classification (tumor present/absent) and tumor segmentation</li> <li>Input: 3D MRI scans (256x256x128 voxels)</li> <li>Model Architecture: 3D U-Net implemented in PyTorch</li> <li>GPU: NVIDIA Tesla V100</li> <li>Docker Image: brain-mri-model:v1</li> </ul>"},{"location":"blog/deployment/latency_and_throughput/#latency-requirements","title":"Latency Requirements:","text":"<ul> <li>Single-scan processing time: &lt; 10 seconds</li> </ul>"},{"location":"blog/deployment/latency_and_throughput/#throughput-goal","title":"Throughput Goal:","text":"<ul> <li>Process at least 300 scans per hour</li> </ul>"},{"location":"blog/deployment/latency_and_throughput/#docker-setup","title":"Docker Setup:","text":"<ol> <li> <p>Ensure the Docker image is built with GPU support:    <pre><code> # Use NVIDIA CUDA base image\n FROM nvidia/cuda:11.0-base\n\n # Set working directory\n WORKDIR /app\n\n # Install Python and pip\n RUN apt-get update &amp;&amp; apt-get install -y \\\n     python3 \\\n     python3-pip \\\n     &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n # Install PyTorch with CUDA support\n RUN pip3 install torch torchvision torchaudio\n\n # Install other dependencies\n COPY requirements.txt .\n RUN pip3 install -r requirements.txt\n\n # Copy model files\n COPY model/ ./model/\n COPY app.py .\n\n # Expose port for the API\n EXPOSE 8080\n\n # Run the application\n CMD [\"python3\", \"app.py\"]\n</code></pre></p> </li> <li> <p>Run the Docker container with GPU access:    <pre><code>docker run --gpus all -p 8080:8080 brain-mri-model:v1\n</code></pre></p> </li> </ol>"},{"location":"blog/deployment/latency_and_throughput/#benchmarking-process","title":"Benchmarking Process:","text":"<ol> <li>Single-Scan Latency Test:    <pre><code>time curl -X POST -H \"Content-Type: application/json\" \\\n     -d '{\"scan_path\": \"/path/to/test_brain_mri.nii\"}' \\\n     http://localhost:8080/predict\n</code></pre></li> </ol> <p>Result: real 0m7.823s</p> <ol> <li>Throughput Calculation:    <pre><code>latency = 7.823  # seconds\nscans_per_hour = 3600 / latency\nprint(f\"Estimated throughput: {scans_per_hour:.0f} scans per hour\")\n</code></pre></li> </ol> <p>Result: Estimated throughput: 459 scans per hour</p>"},{"location":"blog/deployment/latency_and_throughput/#analysis","title":"Analysis:","text":"<ul> <li>Single-scan latency (7.823s) meets the requirement of &lt;10s.</li> <li>Estimated throughput (459 scans/hour) exceeds the goal of 300 scans/hour.</li> </ul>"},{"location":"blog/deployment/latency_and_throughput/#docker-specific-considerations","title":"Docker-specific Considerations:","text":"<ol> <li> <p>GPU Passthrough: Ensure that Docker is configured to use the GPU. This might require installing the NVIDIA Container Toolkit.</p> </li> <li> <p>Resource Limits: Set appropriate CPU and memory limits in your Docker run command to ensure consistent performance:    <pre><code>docker run --gpus all --cpus 4 --memory 16g -p 8080:8080 brain-mri-model:v1\n</code></pre></p> </li> <li>Docker Image Optimization: Use multi-stage builds to keep the final image size small, which can improve container start-up time.</li> </ol>"},{"location":"blog/deployment/latency_and_throughput/#deployment-considerations","title":"Deployment Considerations:","text":"<ul> <li>Ensure the production environment supports Docker with GPU acceleration.</li> <li>Implement a load balancer to distribute requests across multiple Docker containers for higher throughput.</li> </ul> <p>By conducting these benchmarks and analyses on your Dockerized model, you can ensure that your deployed medical imaging ML model meets the required performance standards for clinical use, while benefiting from the portability and consistency that Docker provides.</p>"},{"location":"blog/introduction/overview/","title":"MLOps: A Prescription for Precise, Reliable, and Meaningful Medical Imaging Insights","text":"<p>Machine Learning Operations, or MLOps, is an innovative and rapidly evolving practice that focuses on the streamlined development, deployment, and management of machine learning models. In the realm of medical imaging, where precision and reliability are of utmost importance, MLOps plays a critical role in enhancing the efficacy and trustworthiness of machine learning solutions.</p>"},{"location":"blog/introduction/overview/#precision-reliability-pillars-of-trust-in-medical-imaging","title":"Precision &amp; Reliability: Pillars of Trust in Medical Imaging","text":"<p>In the context of medical imaging, precision and reliability are not just technical requirements\u2014they are fundamental to patient care. Precision refers to the model's ability to minimize false positives, ensuring that healthcare professionals can trust the model's predictions without undue concern over incorrect diagnoses. Reliability emphasizes the consistent and repeatable performance of models, which is essential for maintaining trust in automated systems within clinical settings. The combination of these two attributes is crucial because incorrect predictions can lead to severe consequences, including misdiagnosis or unnecessary treatments, ultimately affecting patient outcomes and eroding trust in AI-based diagnostic tools.</p> <p>Trust is doubly important in medical imaging, as it forms the foundation by which patients rely on medical practitioners, who in turn need to trust the tools they are using to deliver care. </p> \ud83c\udfaf Precision vs. AccuracyWhile accuracy measures the overall correctness of a model, precision specifically targets the rate of false positives, which is crucial in medical imaging to prevent unwarranted alarms. For example, in detecting tumors, high precision ensures that healthy tissue is not mistakenly flagged as cancerous, avoiding undue stress and further invasive tests for patients.      \ud83d\udee1\ufe0f Reliability vs. Inference TimeAlthough fast inference time is valuable for real-time diagnosis, it's secondary to ensuring models consistently produce reliable results. Reliable models minimize the risk of fluctuating predictions, which could lead to contradictory diagnoses over time, causing confusion for healthcare providers and patients alike.      \u23f3 Inference Time Still MattersWhile reliability typically governs the effectiveness of an ML model, inference time may hold more weight in different medical contexts: such as stroke detection, where time is brain.      <p>By integrating MLOps into medical imaging projects, organizations can achieve several critical objectives:</p> <ol> <li>Seamless Model Deployment and Robust Monitoring: MLOps ensures that machine learning models are deployed smoothly and monitored continuously. This ongoing monitoring allows for immediate identification and rectification of issues, enabling continuous model improvement and reliable delivery.</li> <li>Adherence to Regulatory Standards: The healthcare industry is highly regulated to ensure patient safety and data privacy. MLOps helps organizations comply with these stringent regulatory standards, thereby minimizing patient risk and ensuring that AI tools meet the required ethical and legal guidelines.</li> <li>Consistent Delivery of Precise and Reliable Results: As clinical environments evolve, so too must silicon \u2014 i.e. machine learning models. MLOps facilitates the maintenance and updating of models to ensure they continue to deliver accurate and dependable results, adapting to new data and changing conditions within healthcare settings.</li> </ol>"},{"location":"blog/introduction/overview/#driving-innovation-while-minimizing-risks","title":"Driving Innovation While Minimizing Risks","text":"<p>The integration of MLOps into medical imaging not only minimizes risks but also drives innovation within AI-assisted healthcare. This approach enhances the overall quality of care by ensuring that machine learning models are both reliable and precise, ultimately improving patient outcomes and fostering greater trust in AI technologies.</p> <p>By embracing MLOps, the medical imaging field can advance toward a future where AI-driven tools are an integral part of everyday clinical practice, delivering high-quality, reliable, and precise care to patients worldwide.</p>"},{"location":"blog/post-deployment-surveillance/gpt4example/","title":"Postmarket Surveillance of ML Model Pipelines in Medical Imaging: An MLOps Perspective","text":""},{"location":"blog/post-deployment-surveillance/gpt4example/#audience","title":"Audience","text":"<p>This post is intended for healthcare professionals, medical imaging specialists, data scientists, and AI/ML engineers who are involved in the development, deployment, and maintenance of machine learning (ML) models in medical imaging.</p>"},{"location":"blog/post-deployment-surveillance/gpt4example/#what-is-post-deployment-surveillance","title":"What is Post-Deployment Surveillance?","text":"<p>Post-deployment surveillance is the continuous monitoring and evaluation of ML models once they are deployed in a clinical setting. This process ensures that the models perform as expected and continue to provide accurate and reliable results for medical image analysis.</p>"},{"location":"blog/post-deployment-surveillance/gpt4example/#why-is-it-necessary","title":"Why is it Necessary?","text":""},{"location":"blog/post-deployment-surveillance/gpt4example/#model-drift","title":"Model Drift","text":"<p>Model drift is a critical challenge in maintaining the performance of ML models over time. It can manifest in two main forms: data drift and concept drift.</p>"},{"location":"blog/post-deployment-surveillance/gpt4example/#data-drift","title":"Data Drift","text":"<p>Data drift refers to the change in the distribution of serving data compared to the training data.</p>"},{"location":"blog/post-deployment-surveillance/gpt4example/#example","title":"Example:","text":"<ul> <li>Adjustments in scan parameters due to hardware upgrades or time constraints can significantly alter the input data.</li> </ul>"},{"location":"blog/post-deployment-surveillance/gpt4example/#detecting-drift","title":"Detecting drift:","text":"<ul> <li>Subgroup Assessment: Identify specific subgroups (e.g., gender, age, image slice thickness) to monitor differences in distribution.</li> <li>Examine outliers and perform a comparative analysis against the larger dataset.</li> <li>Utilize statistical methods like k-means clustering to understand performance across parameters.</li> <li>Metric and Threshold Definition: Establish metrics and thresholds to quantify and detect drift.</li> <li>For instance, a shift in the demographic distribution from the training to serving data should trigger an alert.</li> <li>Action Plan Development: Create protocols for responding to detected drift.</li> <li>Engage with end-users to determine the impact on model performance.</li> <li>Acquire new data to address gaps, such as underrepresented groups.</li> <li>Anticipation of Drift Scenarios: Predict common scenarios where drift may occur.</li> <li>Stay informed about changes in imaging protocols or the adoption of new medication guidelines that could affect data.</li> </ul>"},{"location":"blog/post-deployment-surveillance/gpt4example/#concept-drift","title":"Concept Drift","text":"<p>Concept drift occurs when the definition or interpretation of labels in serving data evolves.</p>"},{"location":"blog/post-deployment-surveillance/gpt4example/#example_1","title":"Example:","text":"<ul> <li>The clinical definition of certain pathologies, such as edema in brain tumor MRIs, may be revised to reflect new medical insights or standards.</li> </ul>"},{"location":"blog/post-deployment-surveillance/gpt4example/#overcoming-concept-drift","title":"Overcoming concept drift:","text":"<ul> <li>Post-Inference Adaptation: Modify software to recognize and classify new definitions in the post-inference phase.</li> <li>Model Retraining: Periodically update models to align with the current clinical definitions and practices.</li> <li>Continuous Monitoring: Implement regular reviews to quickly identify and respond to concept drift.</li> </ul>"},{"location":"blog/post-deployment-surveillance/gpt4example/#from-an-mlops-perspective","title":"From an MLOps Perspective","text":"<p>Ensuring the robustness of ML models in the medical imaging space through post-deployment surveillance is an essential component of MLOps. The following MLOps strategies are vital:</p> <ul> <li>Automated Monitoring: Use automated systems to monitor model performance and data quality in real-time.</li> <li>Versioning Data and Models: Keep meticulous records of data and model versions to track changes and facilitate rollbacks if necessary.</li> <li>Continuous Training Pipelines: Develop pipelines that can continuously train models with new data as it becomes available.</li> <li>Collaboration with Clinicians: Maintain a close collaboration with medical professionals to understand clinical changes that might affect model performance.</li> </ul> <p>By integrating these MLOps practices, healthcare organizations can ensure that their ML models remain accurate, reliable, and effective in improving patient outcomes through advanced medical imaging analysis.</p>"},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/","title":"Postmarket Deployment Section","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/#audience","title":"Audience","text":"<p>This post is for some humans.</p>"},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/#what-is-post-deployment-surveillance","title":"What is post-deployment surveillance","text":""},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/#why-is-it-necessary","title":"Why is it necessary?","text":""},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/#model-drift","title":"Model Drift","text":"<p>There are two main types of model drift: data drift and concept drift.</p>"},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/#data-drift","title":"Data Drift","text":"<p>Changes in distribution of serving data relative to training data over time.</p> <p>Example:  - Scan Parameters are adjusted due to changes in scanner hardware, or scan time contraints.   </p> <p>Detecting drift:</p> <ul> <li> <p>Define which subgroups to assess (e.g. gender, age, image slice thickness)</p> <ul> <li>difference in distribution of parameters between outliers and rest of data</li> <li>split dataset into two, four, etc. --&gt; looking at subdistribution difference of a given variable</li> <li>k-means clustering across all parameters and performance</li> <li>this can help flag serving data that may yield subpar performance</li> <li>this also helps with targeted data sharing asks - help define data requests from users, if an option</li> </ul> </li> <li> <p>Define metric(s) and threshold(s) to measure and detect drift.</p> <ul> <li>e.g. distribution of Female:Male in training data is significantly different than that seen in serving data </li> </ul> </li> <li> <p>Define what actions to take if drift is detected. </p> <ul> <li> <p>Determine whether drift affects serving performance by reaching out to users.</p> </li> <li> <p>Obtain new data that doesn't overlap with current model's training distribution (e.g. males under 50) from impacted users, if possible.</p> </li> </ul> </li> <li> <p>Anticipate most likely situations drift can occur</p> <ul> <li>New guidelines for imaging protocol optimization may trickle down to imaging centers while new medications that requires a specific, nuanced protocol may be more quickly adopted.</li> </ul> </li> </ul>"},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/#concept-drift","title":"Concept Drift","text":"<p>Changes in meaning or definition of serving data labels change. A classic example in ML is that of certain long-form emails with many attachments originally labeled spam are now considered not spam.</p> <p>In medical imaging, not only might scanner parameters change (as in data drift) but the standard definition for a positive label on those images as well.</p> <p>Example: - the standard definition of an edema label that was applied to brain tumor MRIs changes to include/not include certain positive FLAIR signal in a specific location. - microhemorrhages are re-classified or sub-classified based on size, and this becomes the standard in T2*GRE/SWI image reading.</p> <p>How might you overcome concept drift? Depending on the specific scenario and extent of definition change, adapting to concept drift may be possible. For example, in the case of the microhemorrhage mentioned above, perhaps lesions below a certain diameter of 5mm are sub-classified as \"small\" and physicians may expect this from any image detection model applied to microhemorrhages. Software can be designed to detect segmentation masks that fall into this \"new\" definition or class and apply this label at a post-inference step. Eventually, model retraining / tuning may be necessary to maintain performance if concepts drift beyond the scope of what simple post-inference logic can fix.</p> <p>E.g. </p> <p>how to detect drift? Sample-wise distribution deviation detection how different are the metadata and/or model output data from the training data? Which data to collect? What would be useful? For example, comparing age, sex, image protocol DICOM tags, site location, and other metadata from sample in the field to training distribution. Is the sample out-of-distribution? If so, this sample can be flagged for further review. Potentially, pending data-sharing rules, can be labeled appropriately and added to an updated training set. Scrape logs for info Output json of metrics</p>"},{"location":"blog/post-deployment-surveillance/post-deployment-surveillance/#more-customizability-w-html","title":"more customizability w html","text":"Brain mets detected by DL model."}]}